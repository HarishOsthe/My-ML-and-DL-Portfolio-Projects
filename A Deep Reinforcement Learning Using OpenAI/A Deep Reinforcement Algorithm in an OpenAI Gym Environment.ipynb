{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13.1\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "print(gym.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the openAI gym\n",
    "\n",
    "env=gym.make('CartPole-v0')\n",
    "\n",
    "for i_episode in range(20):\n",
    "    observation=env.reset()\n",
    "    for t in range(1000):\n",
    "        env.render()\n",
    "#         print(observation)\n",
    "        action=env.action_space.sample()\n",
    "        observation, reward, done, info=env.step(action)\n",
    "        \n",
    "#         if done:\n",
    "#             print(f'Eposide finished after {t+1} timesteps')\n",
    "#             break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is our enviroment and what we want to is to teach the renforcement network to apply a force to left or to right so that it balances that pole on top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n",
      "Box(4,)\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The discreate space allows a fixed range of non-negative numbers and in this case the allowed actions are going to be either 0 or 1 and the box space represents an N dimensional box and valid obsevations for this case wiil be an array of 4 numbers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n",
      "[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space.high)\n",
    "print(env.observation_space.low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This are the possible ranges of the observations, these are probably specifying things like location of the cart, location of the pole, we have two actions here 1 going to be force left or right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes=1000\n",
    "n_win_ticks= 195\n",
    "max_env_steps=None\n",
    "\n",
    "gamma=1.0\n",
    "epsilon=1.0\n",
    "epsilon_min=0.01\n",
    "epsilon_decay=0.995\n",
    "alpha=0.01\n",
    "alpha_decay=0.01\n",
    "\n",
    "batch_size=64\n",
    "monitor=False\n",
    "quiet=False\n",
    "\n",
    "#Enviroment Paramenters\n",
    "memory=deque(maxlen=100000)\n",
    "env=gym.make('CartPole-v0')\n",
    "\n",
    "if max_env_steps is not None:\n",
    "    env.max_episode_steps=max_env_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model.add(Dense(24,input_dim=4,activation='relu'))\n",
    "model.add(Dense(48,activation='relu'))\n",
    "model.add(Dense(2,activation='relu'))\n",
    "model.compile(loss='mse',optimizer=Adam(lr=alpha, decay=alpha_decay))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define necessary functions\n",
    "\n",
    "def remember(state,action,reward,next_state,done):\n",
    "    memory.append((state,action,reward,next_state,done))\n",
    "    \n",
    "def choose_action(state,epsilon):\n",
    "    return env.action_space.sample() if (np.random.random() <= epsilon) else np.argmax(model.predict(state))\n",
    "\n",
    "def get_epsilon(t):\n",
    "    return max(psilon_min,min(epsilon,1.0 - math.logic((t+1)*epsilon_decay)))\n",
    "\n",
    "def preprocess_state(state):\n",
    "    return np.reshape(state,[1,4])\n",
    "\n",
    "def replay(batch_size,epsilon):\n",
    "    x_batch,y_batch=[],[]\n",
    "    minibatch=random.sample(memory,min(len(memory),batch_size))\n",
    "    \n",
    "    for state, action, reward, next_state, done in minibatch:\n",
    "        y_target=model.predict(state)\n",
    "        y_target[0][action]=reward if done else reward + gamma * np.max(model.predict(next_state)[0])\n",
    "        x_batch.append(state[0])\n",
    "        y_batch.append(y_target[0])\n",
    "        \n",
    "    model.fit(np.array(x_batch),np.array(y_batch),batch_size=len(x_batch),verbose=0)\n",
    "    \n",
    "    \n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon*=epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The main run function\n",
    "\n",
    "def run():\n",
    "    scores=deque(maxlen=100)\n",
    "    \n",
    "    for e in range(n_episodes):\n",
    "        state=preprocess_state(env.reset())\n",
    "        done=False\n",
    "        i=0\n",
    "        while not done:\n",
    "            action =choose_action(state, get_epsilon(e))\n",
    "            next_state,reward,done,_=env.step(action)\n",
    "            env.render()\n",
    "            next_state=preprocess_state(next_state)\n",
    "            remember(state,action,reward,next_state,done)\n",
    "            state=next_state\n",
    "            i+=1\n",
    "            \n",
    "        scores.append(i)\n",
    "        mean_score=np.mean(scores)\n",
    "        \n",
    "        if mean_score >= n_win_ticks and e >= 100:\n",
    "            if not quiet:\n",
    "                print(f'Ran {e} episodes. Solved after {e-100} trails')\n",
    "                return e - 100\n",
    "            \n",
    "            if e % 20 == 0 and not quiet:\n",
    "                print(f'Episodes {e} - mean survival time over last 100 episodes was {mean_score} ticks')\n",
    "                \n",
    "                \n",
    "                \n",
    "            replay(batch_size,epsilon)\n",
    "    if not quiet:\n",
    "        print(f'Did not Solve after {e} wpisodes')\n",
    "        return e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just copying everything done above into a single cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episodes 0 - mean survival time over last 100 episodes was 23.0 ticks\n",
      "Episodes 20 - mean survival time over last 100 episodes was 76.0952380952381 ticks\n",
      "Episodes 40 - mean survival time over last 100 episodes was 64.58536585365853 ticks\n",
      "Episodes 60 - mean survival time over last 100 episodes was 61.52459016393443 ticks\n",
      "Episodes 80 - mean survival time over last 100 episodes was 58.22222222222222 ticks\n",
      "Episodes 100 - mean survival time over last 100 episodes was 58.39 ticks\n",
      "Episodes 120 - mean survival time over last 100 episodes was 52.05 ticks\n",
      "Episodes 140 - mean survival time over last 100 episodes was 50.13 ticks\n",
      "Episodes 160 - mean survival time over last 100 episodes was 49.2 ticks\n",
      "Episodes 180 - mean survival time over last 100 episodes was 49.09 ticks\n",
      "Episodes 200 - mean survival time over last 100 episodes was 49.19 ticks\n",
      "Episodes 220 - mean survival time over last 100 episodes was 50.3 ticks\n",
      "Episodes 240 - mean survival time over last 100 episodes was 50.81 ticks\n",
      "Episodes 260 - mean survival time over last 100 episodes was 49.99 ticks\n",
      "Episodes 280 - mean survival time over last 100 episodes was 48.82 ticks\n",
      "Episodes 300 - mean survival time over last 100 episodes was 46.06 ticks\n",
      "Episodes 320 - mean survival time over last 100 episodes was 42.62 ticks\n",
      "Episodes 340 - mean survival time over last 100 episodes was 43.14 ticks\n",
      "Episodes 360 - mean survival time over last 100 episodes was 42.39 ticks\n",
      "Episodes 380 - mean survival time over last 100 episodes was 40.87 ticks\n",
      "Episodes 400 - mean survival time over last 100 episodes was 38.61 ticks\n",
      "Episodes 420 - mean survival time over last 100 episodes was 38.06 ticks\n",
      "Episodes 440 - mean survival time over last 100 episodes was 34.89 ticks\n",
      "Episodes 460 - mean survival time over last 100 episodes was 32.27 ticks\n",
      "Episodes 480 - mean survival time over last 100 episodes was 30.25 ticks\n",
      "Episodes 500 - mean survival time over last 100 episodes was 27.76 ticks\n",
      "Episodes 520 - mean survival time over last 100 episodes was 25.62 ticks\n",
      "Episodes 540 - mean survival time over last 100 episodes was 23.36 ticks\n",
      "Episodes 560 - mean survival time over last 100 episodes was 21.63 ticks\n",
      "Episodes 580 - mean survival time over last 100 episodes was 20.81 ticks\n",
      "Episodes 600 - mean survival time over last 100 episodes was 19.96 ticks\n",
      "Episodes 620 - mean survival time over last 100 episodes was 18.51 ticks\n",
      "Episodes 640 - mean survival time over last 100 episodes was 17.15 ticks\n",
      "Episodes 660 - mean survival time over last 100 episodes was 15.44 ticks\n",
      "Episodes 680 - mean survival time over last 100 episodes was 13.69 ticks\n",
      "Episodes 700 - mean survival time over last 100 episodes was 12.5 ticks\n",
      "Episodes 720 - mean survival time over last 100 episodes was 11.46 ticks\n",
      "Episodes 740 - mean survival time over last 100 episodes was 10.56 ticks\n",
      "Episodes 760 - mean survival time over last 100 episodes was 9.96 ticks\n",
      "Episodes 780 - mean survival time over last 100 episodes was 9.6 ticks\n",
      "Episodes 800 - mean survival time over last 100 episodes was 9.48 ticks\n",
      "Episodes 820 - mean survival time over last 100 episodes was 9.45 ticks\n",
      "Episodes 840 - mean survival time over last 100 episodes was 9.4 ticks\n",
      "Episodes 860 - mean survival time over last 100 episodes was 9.39 ticks\n",
      "Episodes 880 - mean survival time over last 100 episodes was 9.38 ticks\n",
      "Episodes 900 - mean survival time over last 100 episodes was 9.36 ticks\n",
      "Episodes 920 - mean survival time over last 100 episodes was 9.35 ticks\n",
      "Episodes 940 - mean survival time over last 100 episodes was 9.33 ticks\n",
      "Episodes 960 - mean survival time over last 100 episodes was 9.34 ticks\n",
      "Episodes 980 - mean survival time over last 100 episodes was 9.35 ticks\n",
      "Did not Solve after 999 wpisodes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "999"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the network\n",
    "\n",
    "import random\n",
    "import gym\n",
    "import math\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "n_episodes=1000\n",
    "n_win_ticks= 195\n",
    "max_env_steps=None\n",
    "\n",
    "gamma=1.0\n",
    "epsilon=1.0\n",
    "epsilon_min=0.01\n",
    "epsilon_decay=0.995\n",
    "alpha=0.01\n",
    "alpha_decay=0.01\n",
    "\n",
    "batch_size=64\n",
    "monitor=False\n",
    "quiet=False\n",
    "\n",
    "#Enviroment Paramenters\n",
    "memory=deque(maxlen=100000)\n",
    "env=gym.make('CartPole-v0')\n",
    "\n",
    "if max_env_steps is not None:\n",
    "    env.max_episode_steps=max_env_steps\n",
    "\n",
    "from collections import deque    \n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "model=Sequential()\n",
    "model.add(Dense(24,input_dim=4,activation='relu'))\n",
    "model.add(Dense(48,activation='relu'))\n",
    "model.add(Dense(2,activation='relu'))\n",
    "model.compile(loss='mse',optimizer=Adam(lr=alpha, decay=alpha_decay))\n",
    "\n",
    "#define necessary functions\n",
    "\n",
    "def remember(state,action,reward,next_state,done):\n",
    "    memory.append((state,action,reward,next_state,done))\n",
    "    \n",
    "def choose_action(state,epsilon):\n",
    "    return env.action_space.sample() if (np.random.random() <= epsilon) else np.argmax(model.predict(state))\n",
    "\n",
    "def get_epsilon(t):\n",
    "    return max(epsilon_min,min(epsilon,1.0 - math.log10((t+1)*epsilon_decay)))\n",
    "\n",
    "def preprocess_state(state):\n",
    "    return np.reshape(state,[1,4])\n",
    "\n",
    "def replay(batch_size,epsilon):\n",
    "    x_batch,y_batch=[],[]\n",
    "    minibatch=random.sample(memory,min(len(memory),batch_size))\n",
    "    \n",
    "    for state, action, reward, next_state, done in minibatch:\n",
    "        y_target=model.predict(state)\n",
    "        y_target[0][action]=reward if done else reward + gamma * np.max(model.predict(next_state)[0])\n",
    "        x_batch.append(state[0])\n",
    "        y_batch.append(y_target[0])\n",
    "        \n",
    "    model.fit(np.array(x_batch),np.array(y_batch),batch_size=len(x_batch),verbose=0)\n",
    "    \n",
    "    \n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon*=epsilon_decay\n",
    "\n",
    "# The main run function\n",
    "\n",
    "def run():\n",
    "    scores=deque(maxlen=100)\n",
    "    \n",
    "    for e in range(n_episodes):\n",
    "        state=preprocess_state(env.reset())\n",
    "        done=False\n",
    "        i=0\n",
    "        while not done:\n",
    "            action =choose_action(state, get_epsilon(e))\n",
    "            next_state,reward,done,_=env.step(action)\n",
    "            env.render()\n",
    "            next_state=preprocess_state(next_state)\n",
    "            remember(state,action,reward,next_state,done)\n",
    "            state=next_state\n",
    "            i+=1\n",
    "            \n",
    "        scores.append(i)\n",
    "        mean_score=np.mean(scores)\n",
    "        \n",
    "        if mean_score >= n_win_ticks and e >= 100:\n",
    "            if not quiet:\n",
    "                print(f'Ran {e} episodes. Solved after {e-100} trails')\n",
    "                return e - 100\n",
    "            \n",
    "        if e % 20 == 0 and not quiet:\n",
    "            print(f'Episodes {e} - mean survival time over last 100 episodes was {mean_score} ticks')\n",
    "                \n",
    "                \n",
    "                \n",
    "            replay(batch_size,get_epsilon(e))\n",
    "    if not quiet:\n",
    "        print(f'Did not Solve after {e} wpisodes')\n",
    "        return e\n",
    "\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
